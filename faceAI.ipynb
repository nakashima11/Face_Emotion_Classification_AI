{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "EYvcECEIJLzB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wdVlBI8v2oYx",
        "outputId": "0a7deec9-e3d9-41e2-aacc-a2da3cc8ce9e"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7BS8TEI3Y8Qi"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "from torch.utils.data import DataLoader\n",
        "from torchvision.datasets import ImageFolder\n",
        "import matplotlib.pyplot as plt\n",
        "import os\n",
        "\n",
        "# デバイス設定\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# データセットの前処理\n",
        "transform = transforms.Compose([\n",
        "    transforms.Resize((48, 48)),\n",
        "    transforms.Grayscale(),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.5,), (0.5,))\n",
        "])\n",
        "\n",
        "data_dir = \"/content/drive/Othercomputers/マイ iMac/2024_中島/visual studio/archive\"\n",
        "train_dataset = ImageFolder(os.path.join(data_dir, \"train\"), transform=transform)\n",
        "test_dataset = ImageFolder(os.path.join(data_dir, \"test\"), transform=transform)\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
        "test_loader = DataLoader(test_dataset, batch_size=64, shuffle=False)\n",
        "\n",
        "# モデル定義\n",
        "class FacialExpressionCNN(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(FacialExpressionCNN, self).__init__()\n",
        "        self.conv_layers = nn.Sequential(\n",
        "            nn.Conv2d(1, 32, kernel_size=3, padding=1),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d(2, 2),\n",
        "\n",
        "            nn.Conv2d(32, 64, kernel_size=3, padding=1),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d(2, 2),\n",
        "\n",
        "            nn.Conv2d(64, 128, kernel_size=3, padding=1),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d(2, 2)\n",
        "        )\n",
        "        self.fc_layers = nn.Sequential(\n",
        "            nn.Linear(128 * 6 * 6, 512),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(0.5),\n",
        "            nn.Linear(512, 7)  # 7つの表情クラス\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.conv_layers(x)\n",
        "        x = x.view(-1, 128 * 6 * 6)\n",
        "        x = self.fc_layers(x)\n",
        "        return x\n",
        "\n",
        "# モデル、損失関数、最適化関数の定義\n",
        "model = FacialExpressionCNN().to(device)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
        "\n",
        "# 訓練ループ\n",
        "epochs = 1\n",
        "for epoch in range(epochs):\n",
        "    model.train()\n",
        "    running_loss = 0.0\n",
        "    for images, labels in train_loader:\n",
        "        images, labels = images.to(device), labels.to(device)\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(images)\n",
        "        loss = criterion(outputs, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        running_loss += loss.item()\n",
        "    print(f\"Epoch [{epoch+1}/{epochs}], Loss: {running_loss/len(train_loader):.4f}\")\n",
        "\n",
        "# テスト\n",
        "model.eval()\n",
        "correct = 0\n",
        "total = 0\n",
        "with torch.no_grad():\n",
        "    for images, labels in test_loader:\n",
        "        images, labels = images.to(device), labels.to(device)\n",
        "        outputs = model(images)\n",
        "        _, predicted = torch.max(outputs.data, 1)\n",
        "        total += labels.size(0)\n",
        "        correct += (predicted == labels).sum().item()\n",
        "\n",
        "\n",
        "rint(f\"Test Accuracy: {100 * correct / total:.2f}%\")\n",
        "\n",
        "# サンプル画像の表示と予測\n",
        "dataiter = iter(test_loader)\n",
        "images, labels = next(dataiter)\n",
        "images, labels = images.to(device), labels.to(device)\n",
        "outputs = model(images)\n",
        "_, predicted = torch.max(outputs, 1)\n",
        "\n",
        "fig, axes = plt.subplots(1, 6, figsize=(15, 3))\n",
        "for i in range(6):\n",
        "    ax = axes[i]\n",
        "    img = images[i].cpu().numpy().squeeze()\n",
        "    ax.imshow(img, cmap='gray')\n",
        "    ax.set_title(f\"Pred: {predicted[i].item()} (True: {labels[i].item()})\")\n",
        "    ax.axis('off')\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import cv2\n",
        "import torch\n",
        "import torchvision.transforms as transforms\n",
        "from torchvision.datasets import ImageFolder\n",
        "import numpy as np\n",
        "\n",
        "\n",
        "# モデル読み込み\n",
        "model = FacialExpressionCNN()\n",
        "model.load_state_dict(torch.load('facial_expression_model.pth', map_location=torch.device('cpu')))\n",
        "model.eval()\n",
        "\n",
        "# クラスラベル\n",
        "class_labels = ['Angry', 'Disgust', 'Fear', 'Happy', 'Sad', 'Surprise', 'Neutral']\n",
        "\n",
        "# 画像前処理\n",
        "transform = transforms.Compose([\n",
        "    transforms.Grayscale(),\n",
        "    transforms.Resize((48, 48)),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.5,), (0.5,))\n",
        "])\n",
        "\n",
        "# カメラ起動\n",
        "cap = cv2.VideoCapture(0)\n",
        "face_cascade = cv2.CascadeClassifier(cv2.data.haarcascades + 'haarcascade_frontalface_default.xml')\n",
        "\n",
        "while True:\n",
        "    ret, frame = cap.read()\n",
        "    if not ret:\n",
        "        break\n",
        "\n",
        "    gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
        "    faces = face_cascade.detectMultiScale(gray, 1.3, 5)\n",
        "\n",
        "    for (x, y, w, h) in faces:\n",
        "        face = gray[y:y + h, x:x + w]\n",
        "        face_resized = cv2.resize(face, (48, 48))\n",
        "        face_tensor = transform(face_resized).unsqueeze(0)\n",
        "\n",
        "        with torch.no_grad():\n",
        "            output = model(face_tensor)\n",
        "            prediction = torch.argmax(output, dim=1).item()\n",
        "            emotion = class_labels[prediction]\n",
        "\n",
        "        # 顔枠と予測結果を描画\n",
        "        cv2.rectangle(frame, (x, y), (x + w, y + h), (255, 0, 0), 2)\n",
        "        cv2.putText(frame, emotion, (x, y - 10), cv2.FONT_HERSHEY_SIMPLEX, 0.9, (36, 255, 12), 2)\n",
        "\n",
        "    cv2.imshow('Facial Expression Recognition', frame)\n",
        "\n",
        "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
        "        break\n",
        "\n",
        "cap.release()\n",
        "cv2.destroyAllWindows()\n"
      ],
      "metadata": {
        "id": "uY1hNY9Bf0hm"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}